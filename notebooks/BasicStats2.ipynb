{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributions and Estimators\n",
    "\n",
    "G. Richards\n",
    "(2016, 2018, 2020)\n",
    "\n",
    "Resources for this material include Ivezic Sections 3.3-3.5, Karen' Leighly's [Bayesian Statistics Lecture](http://seminar.ouml.org/lectures/bayesian-statistics/), and [Bevington's book](http://hosting.astro.cornell.edu/academics/courses/astro3310/Books/Bevington_opt.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributions\n",
    "\n",
    "If we are attempting to characterize our data in a way that is **parameterized**, then we need a functional form or a **distribution**.  There are many naturally occurring distributions.  The book goes through quite a few of them.  Here we'll just talk about a few basic ones to get us started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "The uniform distribution is perhaps more commonly called a \"top-hat\" or a \"box\" distribution.  It is specified by a mean, $\\mu$, and a width, $W$, where\n",
    "\n",
    "$$p(x|\\mu,W) = \\frac{1}{W}$$\n",
    "\n",
    "over the range $|x-\\mu|\\le \\frac{W}{2}$ and $0$ otherwise.  That says that \"given $\\mu$ AND $W$, the probability of $x$ is $\\frac{1}{W}$\" (as long as we are within a certain range).\n",
    "\n",
    "Since we are used to thinking of a Gaussian as the *only* type of distribution the concept of $\\sigma$ (aside from the width) may seem strange.  But $\\sigma$ as mathematically defined last time applies here and is\n",
    "$$\\sigma = \\frac{W}{\\sqrt{12}}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Note that if you moved the file out of the git repo, this path will be wrong\n",
    "# You will need to change the path, copy the code, etc.\n",
    "# Or perhaps just work in the git repo and change the name of the file before moving it later.\n",
    "%matplotlib inline\n",
    "%run ../code/fig_uniform_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can implement [uniform](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html#scipy.stats.uniform) in `scipy` as follows.  We'll use the methods listed at the bottom of the link to complete the cell: `dist.rvs(size=N)` which produces `N` random draws from the distribution and `dist.pdf(x)` which returns the value of the pdf at a given $x$.  First create a uniform distribution with parameters `loc=0`,  `scale=2`, and `N=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and execute this cell\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "N = ____ #Complete\n",
    "distU = stats.uniform(____,____) #Complete\n",
    "draws = distU.rvs(____) # ten random draws\n",
    "print(draws)\n",
    "p = distU.pdf(____) #pdf evaluated at x=1\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Did you expect that answer for the pdf?  Why?  What would the pdf be if you changed the width to 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Distribution\n",
    "\n",
    "We have already seen that the Gaussian distribution is given by\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "It is also called the **normal distribution** and can be noted by $\\mathscr{N}(\\mu,\\sigma)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the convolution of two Gaussians results in a Gaussian.  So $\\mathscr{N}(\\mu,\\sigma)$ convolved with $\\mathscr{N}(\\nu,\\rho)$ is $\\mathscr{N}(\\mu+\\nu,\\sqrt{\\sigma^2+\\rho^2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run ../code/fig_gaussian_distribution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Uncomment the next line and run this cell; I just want you to know that this magic function exists.\n",
    "# %load ../code/fig_gaussian_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get some practice!  See http://www.astroml.org/book_figures/chapter3/fig_gaussian_distribution.html\n",
    "\n",
    "In the same manner as above, create a [normal distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html?highlight=stats%20norm#scipy.stats.norm) with `loc=100` and `scale=15`.  Produce 10 random draws and determine the probability at `x=145`.\n",
    "\n",
    "This will be helpful to us in our later example with IQ tests, since IQ is distributed (by definition) \n",
    "as $\\mathscr{N}$(mu=100,sigma=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and execute this cell\n",
    "distG = stats.____(____,____) # Normal distribution with mean = 100, stdev = 15\n",
    "draws = ____.____(____) # 10 random draws\n",
    "p = ____.____(____) # pdf evaluated at x=0\n",
    "print(draws)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's plot that.  Have the plot sample the distribution 1000 times from 0 to 200.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Let's play with Gaussians! Or Normal distributions, N(mu,sigma)\n",
    "## see http://www.astroml.org/book_figures/chapter3/fig_gaussian_distribution.html\n",
    "## Example: IQ is (by definition) distributed as N(mu=100,sigma=15)\n",
    "\n",
    "xgrid = np.linspace(____,____,____) # generate distribution for a uniform grid of x values\n",
    "____ = distG.pdf(____)  # this is a function of xgrid\n",
    "\n",
    "# actual plotting\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "plt.plot(xgrid, gaussPDF, ls='-', c='black', label=r'$\\mu=%i,\\ \\sigma=%i$' % (mu, sigma))\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 0.03)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Above we plotted probability density function.  Sometimes what you want instead is the cumulative distribution function, cdf.  The CDF is the integral of pdf from $x'=-\\infty$ to $x'=x$:\n",
    "$${\\rm CDF}(x|\\mu,\\sigma) = \\int_{-\\infty}^{x'} p(x'|\\mu,\\sigma) dx',$$\n",
    "where\n",
    "${\\rm CDF}(\\infty) = 1$.\n",
    "\n",
    "You will get some practice with CDFs in the Data Camp homework assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#The same as above but now with the cdf method\n",
    "gaussCDF = distG.cdf(xgrid)\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "plt.plot(xgrid, gaussCDF, ls='-', c='black', label=r'$\\mu=%i,\\ \\sigma=%i$' % (mu, sigma))\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(-0.01, 1.01)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$CDF(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What fraction of people have IQ>145?  First let's determine that using the theoretical CDF, using the `sf()` method of [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html).  Then we'll try try a brute force method, simulating it using `sampleSize=1000000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#What fraction of people have IQ>145?\n",
    "#cdf is fraction of people with IQ<=145, 1-cdf is IQ>145\n",
    "#sf (survival function) is 1-cdf, which is what we want in this case\n",
    "sf145 = distG.____(____)\n",
    "print(sf145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Did you get 0.13% (or 0.0013)?\n",
    "\n",
    "Basically this is doing the CDF integral in the opposite direction.  Start at $x=\\infty$ and integrate down the curve to the value of interest (here $x=145$), then report the fraction of the time values in that range (145 and above) are expected given the known distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's now look at the same problems using a sample of a million points drawn from N(100,15)\n",
    "sampleSize=____\n",
    "gaussSample = distG.rvs(sampleSize) \n",
    "smartOnes = gaussSample[gaussSample>____] #Extract only those draws with >145\n",
    "FracSmartOnes = 1.0*np.size(smartOnes)/sampleSize\n",
    "print(FracSmartOnes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about the IQ that corresponds to \"one in a million\"?   Here we want the inverse survival function, `isf()`.  Note that the inverse of the cdf is `ppf()`, the percent point function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "OneInAMillionVal = distG.____(____) #Complete\n",
    "\n",
    "print('IQ=', OneInAMillionVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#What is another way you could estimate this with `gaussSample`?\n",
    "#Think about how you can take advantage of the sampleSize we used above.\n",
    "\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian confidence levels\n",
    "\n",
    "The probability of a measurement drawn from a Gaussian distribution that is between $\\mu-a$ and $\\mu+b$ is\n",
    "$$\\int_{\\mu-a}^{\\mu+b} p(x|\\mu,\\sigma) dx.$$\n",
    "For $a=b=1\\sigma$, we get the familar result of 68.3%.  For $a=b=2\\sigma$ it is 95.4%.  So we refer to the range $\\mu \\pm 1\\sigma$ and $\\mu \\pm 2\\sigma$ as the 68% and 95% **confidence limits**, respectively.\n",
    "\n",
    "Note that if your distribution is not Gaussian, then these confidence intervals will be different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can you figure out what the probability is for $-2\\sigma, +4\\sigma$?  Check to see that you get the right answer for the cases above first!\n",
    "\n",
    "You'll get some practice with this in a Data Camp lesson next week (after we talk about Bootstrap and Jackknife estimates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and execute this cell\n",
    "N=10000\n",
    "mu=0\n",
    "sigma=1\n",
    "distN = ____.____(mu, sigma) # Complete\n",
    "xgrid = np.linspace(____,____,N) # Complete\n",
    "dx = (xgrid.max()-xgrid.min())/N\n",
    "prob = distN.pdf(xgrid)*dx\n",
    "print(prob.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could do this a number of different ways.   I did it this way so that we could see what is going on.  Basically using the trapezoidal method, computing the height and the width and summing them up.  We'll do it below with the CDF and check that the answer is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "upper = distN.cdf(4)\n",
    "lower = distN.cdf(-2)\n",
    "p = upper-lower\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Normal\n",
    "\n",
    "If $x$ is Gaussian distributed with $\\mathscr{N}(\\mu,\\sigma)$, then $y=\\exp(x)$ will have a **log-normal** distribution, where the mean of y is $\\exp(\\mu + \\sigma^2/2)$.  Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "x = stats.norm(0,1) # mean = 0, stdev = 1\n",
    "y = np.exp(x.rvs(100))\n",
    "print(y.mean())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The catch here is that stats.norm(0,1) returns an *object* and not something that we can just do math on in the expected manner.  What *can* you do with it?  Try ```dir(x)``` to get a list of all the methods and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Complete and execute this cell\n",
    "distLN = stats.norm(0,1) # mean = 0, stdev = 1\n",
    "x = distLN.rvs(10000)\n",
    "y = np.exp(x)\n",
    "print(math.exp(0+1*1/2.0), y.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\chi^2$ Distribution\n",
    "\n",
    "We'll run into the $\\chi^2$ distribution when we talk about Maximum Likelihood in the next chapter.\n",
    "\n",
    "If we have a Gaussian distribution with values ${x_i}$, and we scale and normalize them according to\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma},$$\n",
    "then the sum of squares, $Q$ \n",
    "$$Q = \\sum_{i=1}^N z_i^2,$$\n",
    "will follow the $\\chi^2$ distribution.  The *number of degrees of freedom*, $k$, is given by the number of data points, $N$ (minus any constraints).  The pdf of $Q$ given $k$ defines $\\chi^2$ and is given by\n",
    "$$p(Q|k)\\equiv \\chi^2(Q|k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}Q^{k/2-1}\\exp(-Q/2),$$\n",
    "where $Q>0$ and the $\\Gamma$ function would just be the usual factorial function if we were dealing with integers, but here we have half integers.\n",
    "\n",
    "This is ugly, but it is really just a formula like anything else.  Note that the shape of the distribution *only* depends on the sample size $N=k$ and not on $\\mu$ or $\\sigma$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run ../code/fig_chi2_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chi-squared per degree of freedom\n",
    "\n",
    "In practice we frequently divide $\\chi^2$ by the number of degrees of freedom, and work with:\n",
    "\n",
    "$$\\chi^2_{dof} = \\frac{1}{N-1} \\sum_{i=1}^N \\left(\\frac{x_i-\\overline{x}}{\\sigma}\\right)^2$$\n",
    "\n",
    "which (for large $k$) is distributed as\n",
    "\n",
    "$$ p(\\chi^2_{dof}) \\sim \\mathscr{N}\\left(1, \\sqrt{\\frac{2}{N-1}}\\right) $$\n",
    "\n",
    "(where $k = N-1$, and $N$ is the number of samples). Therefore, we expect $\\chi^2_{dof}$ to be 1, to within a few $\\sqrt{\\frac{2}{N-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "See the [Khan Academy's chi-square distribution introduction](https://www.khanacademy.org/math/statistics-probability/inference-categorical-data-chi-square-tests/chi-square-goodness-of-fit-tests/v/chi-square-distribution-introduction), which is actually somewhat confusing but shows how to use probability tables (starting at 7:50) to get so-called $p$-values.\n",
    "\n",
    "If you are so inclined, you could look ahead to next week a bit and watch the video on Least-squares regression\n",
    "https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/regression-library/v/introduction-to-residuals-and-least-squares-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Student's $t$ Distribution\n",
    "\n",
    "Another distribution that we'll see later is the Student's $t$ Distribution.\n",
    "\n",
    "If you have a sample of $N$ measurements, $\\{x_i\\}$, drawn from a Gaussian distribution, $\\mathscr{N}(\\mu,\\sigma)$, and you apply the transform\n",
    "$$t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{N}},$$\n",
    "then $t$ will be distributed according to Student's $t$ with the following pdf (for $k$ degrees of freedom): \n",
    "$$p(x|k) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{\\pi k} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}$$\n",
    "\n",
    "As with a Gaussian, Student's $t$ is bell shaped, but has \"heavier\" tails.\n",
    "\n",
    "Note the similarity between $t$ and $z$ for a Gaussian, which reflects the difference between estimates of the mean and standard deviation and their true values.  (Which means that often you should be using a $t$-distribution instead of a normal distribution, but the difference goes away for large enough $k$, so shouldn't matter for \"Big Data\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run ../code/fig_student_t_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's the point?\n",
    "\n",
    "The point is that we are going to make some measurement.  And we will want to know how likely it is that we would get that measurement in our experiment as compared to random chance.  To determine that we need to know the shape of the distribution.  Let's say that we find that $x=6$.  If our data is $\\chi^2$ distributed with 2 degrees of freedom, then we would integrate the $k=2$ curve above from 6 to $\\infty$ to determine how likely it is that we would have gotten 6 or larger by chance.  If our distribution was instead $t$ distributed, we would get a *very* different answer.  Note that it is important that you decide *ahead of time* what the metric will be for deciding whether this result is significant or not.  More on this later, but see [this article](http://fivethirtyeight.com/features/science-isnt-broken/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "One of the reasons that a Gaussian (or Normal) Distribution is so common is because of the **Central Limit Theorem**. It says that for an arbitrary distribution, $h(x)$, that has a well-defined mean, $\\mu$, and standard deviation, $\\sigma$, the mean of $N$ values \\{$x_i$\\} drawn from the distribution will follow a Gaussian Distribution with $\\mathscr{N}(\\mu,\\sigma/\\sqrt{N})$.  (A Cauchy distribution is one example where this fails.)\n",
    "\n",
    "This theorem is the foudation for the performing repeat measurements in order to improve the accuracy of one's experiment.  It is telling us something about the *shape* of the distribution that we get when averaging.  The **Law of Large Numbers** further says that the sample mean will converge to the distribution mean as $N$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Personally, I always find this a bit confusing (or at least I forget how it works).  So, let's look at it in detail.\n",
    "Start by plotting a normal distribution with $\\mu=0.5$ and $\\sigma=1/\\sqrt{12}/\\sqrt{2}$.\n",
    "\n",
    "Now take `N=2` draws using the `np.random.random` distribution and plot them as a rug plot.  Do that a couple of times (e.g., keep hitting Cntrl-Enter in the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "N=____ # Number of draws\n",
    "mu=____ # Location\n",
    "sigma =1.0/np.sqrt(12)/np.sqrt(N) # Sqrt(N) properly normalizes the pdf\n",
    "\n",
    "xgrid = ____.____(____,____,1000) # Array to sample the space \n",
    "distG = stats.norm(mu,sigma) # Complete\n",
    "plt.plot(____,distG.____(____)) # Complete\n",
    "\n",
    "x = np.random.____(____) # Two random draws\n",
    "plt.plot(x, 0*x, '|', markersize=50)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's average those two draws and plot the result (in the same panel).  Do it as a histogram for 1,000,000 samples (of 2 each).  Use a stepfilled histogram that is normalized with 50% transparency and 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "N=____ # Number of draws\n",
    "mu=____ # Location\n",
    "sigma = 1.0/np.sqrt(12)/np.sqrt(N)  # Scale factor\n",
    "\n",
    "xgrid = ____.____(____,____,____) # Array to sample the space\n",
    "distG = ____.____(____,____) # Complete\n",
    "plt.plot(____,____.____(____)) # Complete\n",
    "\n",
    "x = np.____.____(____) # N random draws\n",
    "plt.plot(x, 0*x, '|', markersize=50)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('pdf')\n",
    "\n",
    "# Add a histogram that is the mean of 1,000,000 draws\n",
    "yy = []\n",
    "for i in np.arange(100000):\n",
    "    xx = np.random.random(N) # N random draws\n",
    "    yy.append(____.____()) # Append average of those random draws to the end of the array\n",
    "\n",
    "_ = plt.hist(yy,bins=100,histtype='stepfilled', alpha=0.5, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now instead of averaging 2 draws, average 3.  Then do it for 10.  Then for 100.  Each time for 1,000,000 samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Copy your code from above and edit accordingly (or just edit your code from above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For 100 you will note that your draws are clearly sampling the full range, but the means of those draws are in a *much* more restrictred range.  Moreover they are very closely following a Normal Distribution.  This is the power of the Central Limit Theorem.    We'll see this more later when we talk about **maximum likelihood**.\n",
    "\n",
    "By the way, if your code is ugly, you can run the following cell to reproduce Ivezic, Figure 3.20 which nicely illustrates this in one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%run ../code/fig_central_limit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you are confused, then watch this video from the Khan Academy:\n",
    "[https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem](https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate and Multivariate Distribution Functions\n",
    "\n",
    "Up to now we have been dealing with one-dimensional distribution functions.  Let's now consider a two dimensional distribution $h(x,y)$ where $$\\int_{-\\infty}^{\\infty}dx\\int_{-\\infty}^{\\infty}h(x,y)dy = 1.$$  $h(x,y)$ is telling us the probability that $x$ is between $x$ and $dx$ and *also* that $y$ is between $y$ and $dy$.\n",
    "\n",
    "Then we have the following definitions:\n",
    "\n",
    "$$\\sigma^2_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma^2_y = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(y-\\mu_y)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\mu_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma_{xy} = Cov(x,y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x) (y-\\mu_y) h(x,y) dx dy$$\n",
    "\n",
    "If $x$ and $y$ are uncorrelated, then we can treat the system as two independent 1-D distributions.  This means that choosing a range on one variable has no effect on the distribution of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write a 2-D Gaussian pdf as\n",
    "$$p(x,y|\\mu_x,\\mu_y,\\sigma_x,\\sigma_y,\\sigma_{xy}) = \\frac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}} \\exp\\left(\\frac{-z^2}{2(1-\\rho^2)}\\right),$$\n",
    "\n",
    "where $$z^2 = \\frac{(x-\\mu_x)^2}{\\sigma_x^2} + \\frac{(y-\\mu_y)^2}{\\sigma_y^2} - 2\\rho\\frac{(x-\\mu_x)(y-\\mu_y)}{\\sigma_x\\sigma_y},$$\n",
    "\n",
    "with $$\\rho = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$$\n",
    "as the (dimensionless) correlation coefficient.\n",
    "\n",
    "If $x$ and $y$ are perfectly correlated then $\\rho=\\pm1$ and if they are uncorrelated, then $\\rho=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pdf is now not a histogram, but rather a series of contours in the $x-y$ plane.   These are centered at $(x=\\mu_x, y=\\mu_y)$ and are tilted at angle $\\alpha$, which is given by\n",
    "$$\\tan(2 \\alpha) = 2\\rho\\frac{\\sigma_x\\sigma_y}{\\sigma_x^2-\\sigma_y^2} = 2\\frac{\\sigma_{xy}}{\\sigma_x^2-\\sigma_y^2}.$$\n",
    "\n",
    "For example (Ivezic, Figure 3.22):\n",
    "![Ivezic, Figure 3.22](http://www.astroml.org/_images/fig_bivariate_gaussian_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can define new coordinate axes that are aligned with the minimum and maximum widths of the distribution.  These are called the **principal axes** and are given by\n",
    "$$P_1 = (x-\\mu_x)\\cos\\alpha + (y-\\mu_y)\\sin\\alpha,$$\n",
    "and\n",
    "$$P_2 = -(x-\\mu_x)\\sin\\alpha + (y-\\mu_y)\\cos\\alpha.$$\n",
    "\n",
    "The widths in this coordinate system are\n",
    "$$\\sigma^2_{1,2} = \\frac{\\sigma_x^2+\\sigma_y^2}{2}\\pm\\sqrt{\\left(\\frac{\\sigma_x^2-\\sigma_y^2}{2}\\right)^2 + \\sigma^2_{xy}}.$$\n",
    "\n",
    "Note that the correlation vanishes in this coordinate system (by definition) and the bivariate Gaussian is just a product of two univariate Gaussians.  This concept will be crucial for understanding Principal Component Analysis when we get to Chapter 7, where PCA extends this idea to even more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the univariate case we used $\\overline{x}$ and $s$ to *estimate* $\\mu$ and $\\sigma$.  In the bivariate case we estimate 5 parameters: $(\\overline{x},\\overline{y},s_x,s_y,s_{xy})$.  \n",
    "\n",
    "As with the univariate case, it is important to realize that outliers can bias these estimates and that it may be more appropriate to use the median rather than the mean as a more robust estimator for $\\mu_x$ and $\\mu_y$.  Similarly we want robust estimators for the other parameters of the fit.  We won't go into that in detail right now, but see Ivezic, Figure 3.23 for an example:\n",
    "\n",
    "![Ivezic, Figure 3.23](http://www.astroml.org/_images/fig_robust_pca_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For an example of how to generate a bivariate distribution and plot confidence contours, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Base code drawn from Ivezic, Figure 3.22, edited by G. Richards to simplify the example\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from astroML.stats.random import bivariate_normal\n",
    "from astroML.stats import fit_bivariate_normal\n",
    "\n",
    "mux = 0\n",
    "muy = 0\n",
    "sigx = 1.0\n",
    "sigy = 1.0\n",
    "sigxy = 0.3\n",
    "#------------------------------------------------------------\n",
    "# Create 10,000 points from a multivariate normal distribution\n",
    "mean = [mux, muy]\n",
    "cov = [[sigx, sigxy], [sigxy, sigy]]\n",
    "x, y = np.random.multivariate_normal(mean, cov, 10000).T\n",
    "\n",
    "# Fit those data with a bivariate normal distribution\n",
    "mean, sigma_x, sigma_y, alpha = fit_bivariate_normal(x,y)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(x,y,s=2,edgecolor='none')\n",
    "\n",
    "# draw 1, 2, 3-sigma ellipses over the distribution\n",
    "for N in (1, 2, 3):\n",
    "    ax.add_patch(Ellipse(mean, N * sigma_x, N * sigma_y, angle=alpha * 180./np.pi, lw=1, ec='k', fc='none'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
